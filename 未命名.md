I build the Kubernetes cluster based on the Week 2 Lab and Week 4 Lab.
The provided commands guide me through the detailed installation process of Docker and Kubernetes on an Ubuntu-based server. First, it updates the package lists and installs prerequisites to ensure secure package handling. It then sets up the Docker repository and installs Docker components, including Docker Engine and command-line tools. For Kubernetes, after adding the repository, it installs key components like kubelet, kubeadm, and kubectl, and prevents them from automatic updates with the apt-mark hold command. The specific network plugin for Kubernetes isn't mentioned, which is crucial for defining how pods communicate and are typically selected based on performance and feature needs. Finally, the versions installed are the latest as provided by the Docker repository and Kubernetes stable release channel for version 1.29, ensuring compatibility and modern features are supported.

In Kubernetes, `service.yml` and `deployment.yml` are YAML configuration files used to define how Docker containers should be deployed, managed, and made accessible within a Kubernetes cluster. Hereâ€™s an overview of what typically should be included in each:


### deployment.yml

A deployment configuration (`deployment.yml`) is used to describe a desired state in Kubernetes. This includes settings such as the number of replicas, the container image to use, the necessary commands, and the environment settings. The main components are:

1. **apiVersion**: Specifies the version of the Kubernetes API you are using.
2. **kind**: Specifies the kind of Kubernetes object you want to create; in this case, it's a `Deployment`.
3. **metadata**: Provides data that helps uniquely identify the Deployment, like `name` and `namespace`.
4. **spec**:
   - **replicas**: Defines the number of instances of the application you want running.
   - **selector**: Specifies how the Deployment finds which Pods to manage. It uses a label selector to find those Pods that match the criteria.
   - **template**: Defines the Pods and their behavior:
     - **metadata**: Typically, labels that can be used to connect the Deployment with other resources such as Services.
     - **spec**:
       - **containers**: Lists the containers to be created as part of the Pod:
         - **name**: Name of the container.
         - **image**: Docker image to use.
         - **ports**: Container ports to expose.
         - **env**: Environment variables to set in the container.
         - **resources**: CPU and memory limits and requests.
         - **volumeMounts**: Specifies where and how volumes should be mounted within the container.

### service.yml

A service configuration (`service.yml`) is used to define a logical set of Pods and a policy by which to access them, often described using labels and selectors. This helps in exposing the application running on a set of Pods as a network service. Key elements include:

1. **apiVersion**: Specifies the version of the Kubernetes API you are using.
2. **kind**: Specifies the kind of Kubernetes object; here, it's a `Service`.
3. **metadata**:
   - **name**: Name of the service.
   - **labels**: Key/value pairs for organizing and categorizing the Service.
4. **spec**:
   - **selector**: Matches the labels of the Pods you want to expose.
   - **ports**: 
     - **port**: The port the service is exposed on.
     - **targetPort**: The port the service routes to on the Pod.
   - **type**: Specifies the type of Service (e.g., `ClusterIP`, `NodePort`, `LoadBalancer`).



The Locust client is a load testing tool that allows users to access RESTful API. It simulates some concurrent users, known as 'Swarm' in Locust terminology, to test the performance and scalability of web applications. With its simple and intuitive API, users can easily create complex test scenarios and monitor system performance in real-time through a web interface. Here's two Locust client. One is for Nectar and the other is for Kubernetes master. Then let me show you the procedure by starting a new load test with the number of 1 user. As you can see it runs well and no failure occurs.